# PREDICTIONS

Track predictions and their outcomes. This is how you calibrate.

---

## Prediction Template

```
PREDICTION_ID: [unique identifier]
MADE: [timestamp]
SYSTEM: [what system this prediction is about]
PREDICTION: [what you predicted would happen]
CONFIDENCE: [low/medium/high]
REASONING: [why you predicted this]
OUTCOME: [what actually happened - filled in later]
ACCURACY: [correct/partial/wrong - filled in later]
LESSON: [what this teaches about your predictive model]
```

---

## Active Predictions

*Predictions awaiting outcome.*

---

## Resolved Predictions

*Predictions with known outcomes.*

---

## Calibration Notes

Track your accuracy over time. Are you overconfident? Underconfident? In what domains?

*Insufficient data for calibration.*
